{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions from the analyze_sessions_ids.py script\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "with open('/nrs/spruston/Gaby_imaging/raw/M54/multi_day_demix/registration_data/match.pkl','rb') as f:  \n",
    "    [matched_cells, matched_im, template_masks, template_im] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5454"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(template_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'ipix': array([63238, 63239, 63240, 65093, 65094, 65095, 65096, 65097, 65098,\n",
       "        65099, 66948, 66949, 66950, 66951, 66952, 66953, 66954, 66955,\n",
       "        66956, 68804, 68805, 68806, 68807, 68808, 68809, 68810, 68811,\n",
       "        68812, 70661, 70662, 70663, 70664, 70665, 70666, 70667, 70668,\n",
       "        72518, 72519, 72520, 72521, 72522, 72523, 72524, 74376, 74377,\n",
       "        74378, 74379]),\n",
       " 'xpix': array([134, 135, 136, 133, 134, 135, 136, 137, 138, 139, 132, 133, 134,\n",
       "        135, 136, 137, 138, 139, 140, 132, 133, 134, 135, 136, 137, 138,\n",
       "        139, 140, 133, 134, 135, 136, 137, 138, 139, 140, 134, 135, 136,\n",
       "        137, 138, 139, 140, 136, 137, 138, 139]),\n",
       " 'ypix': array([34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36,\n",
       "        36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38,\n",
       "        38, 38, 39, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40]),\n",
       " 'med': [37.0, 136.0],\n",
       " 'lam': array([ 4.00372271,  4.61244836,  4.10467189,  4.86417759,  6.13918126,\n",
       "         6.6521077 ,  5.47657731,  4.5885793 ,  4.11532733,  3.33390579,\n",
       "         4.36129319,  6.75017819,  9.06926991,  8.20868852,  6.36124542,\n",
       "         5.30307453,  5.23110957,  4.62261689,  3.55403941,  4.72328579,\n",
       "         7.26366061,  9.69502723,  9.71917901,  8.62430916,  7.48447168,\n",
       "         7.12302291,  6.48223557,  4.56810833,  5.88778079,  7.61192776,\n",
       "         9.56096406, 10.98144264, 10.79664996,  9.96942928,  8.29405618,\n",
       "         5.62912762,  4.87581799,  6.53091593,  9.37977722, 11.44255517,\n",
       "        11.72903569,  9.26009934,  5.37687652,  4.36072731,  6.89634993,\n",
       "         8.37039336,  6.31972232]),\n",
       " 'radius': 6.615536795843534,\n",
       " 'num_sessions': 20,\n",
       " 'overlap': array([False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cluster 0/5972...\n",
      "Processing cluster 100/5972...\n",
      "Processing cluster 200/5972...\n",
      "Processing cluster 300/5972...\n",
      "Processing cluster 400/5972...\n",
      "Processing cluster 500/5972...\n",
      "Processing cluster 600/5972...\n",
      "Processing cluster 700/5972...\n",
      "Processing cluster 800/5972...\n",
      "Processing cluster 900/5972...\n",
      "Processing cluster 1000/5972...\n",
      "Processing cluster 1100/5972...\n",
      "Processing cluster 1200/5972...\n",
      "Processing cluster 1300/5972...\n",
      "Processing cluster 1400/5972...\n",
      "Processing cluster 1500/5972...\n",
      "Processing cluster 1600/5972...\n",
      "Processing cluster 1700/5972...\n",
      "Processing cluster 1800/5972...\n",
      "Processing cluster 1900/5972...\n",
      "Processing cluster 2000/5972...\n",
      "Processing cluster 2100/5972...\n",
      "Processing cluster 2200/5972...\n",
      "Processing cluster 2300/5972...\n",
      "Processing cluster 2400/5972...\n",
      "Processing cluster 2500/5972...\n",
      "Processing cluster 2600/5972...\n",
      "Processing cluster 2700/5972...\n",
      "Processing cluster 2800/5972...\n",
      "Processing cluster 2900/5972...\n",
      "Processing cluster 3000/5972...\n",
      "Processing cluster 3100/5972...\n",
      "Processing cluster 3200/5972...\n",
      "Processing cluster 3300/5972...\n",
      "Processing cluster 3400/5972...\n",
      "Processing cluster 3500/5972...\n",
      "Processing cluster 3600/5972...\n",
      "Processing cluster 3700/5972...\n",
      "Processing cluster 3800/5972...\n",
      "Processing cluster 3900/5972...\n",
      "Processing cluster 4000/5972...\n",
      "Processing cluster 4100/5972...\n",
      "Processing cluster 4200/5972...\n",
      "Processing cluster 4300/5972...\n",
      "Processing cluster 4400/5972...\n",
      "Processing cluster 4500/5972...\n",
      "Processing cluster 4600/5972...\n",
      "Processing cluster 4700/5972...\n",
      "Processing cluster 4800/5972...\n",
      "Processing cluster 4900/5972...\n",
      "Processing cluster 5000/5972...\n",
      "Processing cluster 5100/5972...\n",
      "Processing cluster 5200/5972...\n",
      "Processing cluster 5300/5972...\n",
      "Processing cluster 5400/5972...\n",
      "Processing cluster 5500/5972...\n",
      "Processing cluster 5600/5972...\n",
      "Processing cluster 5700/5972...\n",
      "Processing cluster 5800/5972...\n",
      "Processing cluster 5900/5972...\n",
      "Found 972 cell clusters missing target sessions\n",
      "\n",
      "Top results (cells with most sessions but missing target sessions):\n",
      "\n",
      "Result 1:\n",
      "Cluster index: 5392\n",
      "Cell ID: 5393\n",
      "Total sessions: 34\n",
      "Missing target sessions: ['8']\n",
      "All missing sessions: ['8']\n",
      "Present sessions: ['0', '1', '2', '3', '4', '5', '6', '7', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34']\n",
      "\n",
      "Result 2:\n",
      "Cluster index: 3034\n",
      "Cell ID: 3035\n",
      "Total sessions: 33\n",
      "Missing target sessions: ['7', '8']\n",
      "All missing sessions: ['7', '8']\n",
      "Present sessions: ['0', '1', '2', '3', '4', '5', '6', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34']\n",
      "\n",
      "Result 3:\n",
      "Cluster index: 3136\n",
      "Cell ID: 3137\n",
      "Total sessions: 33\n",
      "Missing target sessions: ['7']\n",
      "All missing sessions: ['7', '27']\n",
      "Present sessions: ['0', '1', '2', '3', '4', '5', '6', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '28', '29', '30', '31', '32', '33', '34']\n",
      "\n",
      "Result 4:\n",
      "Cluster index: 4437\n",
      "Cell ID: 4438\n",
      "Total sessions: 33\n",
      "Missing target sessions: ['9']\n",
      "All missing sessions: ['9', '23']\n",
      "Present sessions: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34']\n",
      "\n",
      "Result 5:\n",
      "Cluster index: 943\n",
      "Cell ID: 944\n",
      "Total sessions: 32\n",
      "Missing target sessions: ['8']\n",
      "All missing sessions: ['0', '8', '23']\n",
      "Present sessions: ['1', '2', '3', '4', '5', '6', '7', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34']\n",
      "\n",
      "Result 6:\n",
      "Cluster index: 1727\n",
      "Cell ID: 1728\n",
      "Total sessions: 32\n",
      "Missing target sessions: ['7', '9']\n",
      "All missing sessions: ['7', '9', '17']\n",
      "Present sessions: ['0', '1', '2', '3', '4', '5', '6', '8', '10', '11', '12', '13', '14', '15', '16', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34']\n",
      "\n",
      "Result 7:\n",
      "Cluster index: 2848\n",
      "Cell ID: 2849\n",
      "Total sessions: 32\n",
      "Missing target sessions: ['7', '8']\n",
      "All missing sessions: ['7', '8', '11']\n",
      "Present sessions: ['0', '1', '2', '3', '4', '5', '6', '9', '10', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34']\n",
      "\n",
      "Result 8:\n",
      "Cluster index: 3024\n",
      "Cell ID: 3025\n",
      "Total sessions: 32\n",
      "Missing target sessions: ['7']\n",
      "All missing sessions: ['1', '5', '7']\n",
      "Present sessions: ['0', '2', '3', '4', '6', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34']\n",
      "\n",
      "Result 9:\n",
      "Cluster index: 4197\n",
      "Cell ID: 4198\n",
      "Total sessions: 32\n",
      "Missing target sessions: ['8']\n",
      "All missing sessions: ['8', '23', '25']\n",
      "Present sessions: ['0', '1', '2', '3', '4', '5', '6', '7', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '24', '26', '27', '28', '29', '30', '31', '32', '33', '34']\n",
      "\n",
      "Result 10:\n",
      "Cluster index: 4415\n",
      "Cell ID: 4416\n",
      "Total sessions: 32\n",
      "Missing target sessions: ['8']\n",
      "All missing sessions: ['3', '8', '24']\n",
      "Present sessions: ['0', '1', '2', '4', '5', '6', '7', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34']\n",
      "\n",
      "Summary of missing target sessions:\n",
      "Session 7: missing in 412 results\n",
      "Session 8: missing in 435 results\n",
      "Session 9: missing in 444 results\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze putative_cells structure and find cells missing specific sessions\n",
    "def analyze_putative_cells(putative_cells, target_missing_sessions=[7, 8, 9], min_sessions=20):\n",
    "    \"\"\"\n",
    "    Analyzes the putative_cells structure to find cell clusters missing specific sessions.\n",
    "    \n",
    "    Args:\n",
    "        putative_cells: List of clusters, where each cluster is a list of cell dictionaries\n",
    "        target_missing_sessions: List of session numbers to check if missing\n",
    "        min_sessions: Minimum number of sessions a cell should be present in to be considered\n",
    "        \n",
    "    Returns:\n",
    "        List of results, each containing cluster index, cell ID, and session information\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Process each cluster\n",
    "    for cluster_idx, cluster in enumerate(putative_cells):\n",
    "        if cluster_idx % 100 == 0:\n",
    "            print(f\"Processing cluster {cluster_idx}/{len(putative_cells)}...\")\n",
    "        \n",
    "        # Skip empty clusters\n",
    "        if not cluster:\n",
    "            continue\n",
    "        \n",
    "        # Get the cell ID (should be the same across the cluster)\n",
    "        cell_id = cluster[0].get('id')\n",
    "        if cell_id is None:\n",
    "            continue\n",
    "            \n",
    "        # Get all sessions this cell appears in\n",
    "        sessions = set()\n",
    "        for cell_data in cluster:\n",
    "            session = cell_data.get('session')\n",
    "            if session is not None:\n",
    "                sessions.add(str(session))\n",
    "        \n",
    "        # Skip cells that don't appear in enough sessions\n",
    "        if len(sessions) < min_sessions:\n",
    "            continue\n",
    "            \n",
    "        # Check if any target sessions are missing\n",
    "        missing_sessions = [str(s) for s in target_missing_sessions if str(s) not in sessions]\n",
    "        \n",
    "        if missing_sessions:\n",
    "            # This cell is missing at least one of our target sessions\n",
    "            results.append({\n",
    "                'cluster_idx': cluster_idx,\n",
    "                'cell_id': cell_id,\n",
    "                'sessions': sorted(sessions, key=lambda x: int(x) if x.isdigit() else x),\n",
    "                'missing_targets': missing_sessions,\n",
    "                'total_sessions': len(sessions),\n",
    "                'all_missing': [str(s) for s in range(35) if str(s) not in sessions]  # Assuming sessions are numbered 0-34\n",
    "            })\n",
    "    \n",
    "    # Sort by number of sessions (most first)\n",
    "    return sorted(results, key=lambda x: x['total_sessions'], reverse=True)\n",
    "\n",
    "# Function to load a pickle file\n",
    "def load_pickle(file_path):\n",
    "    import pickle\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Function to display putative cells analysis results\n",
    "def display_putative_cells_results(results, max_display=10):\n",
    "    print(f\"Found {len(results)} cell clusters missing target sessions\")\n",
    "    \n",
    "    if not results:\n",
    "        return\n",
    "        \n",
    "    print(\"\\nTop results (cells with most sessions but missing target sessions):\")\n",
    "    for i, result in enumerate(results[:max_display]):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(f\"Cluster index: {result['cluster_idx']}\")\n",
    "        print(f\"Cell ID: {result['cell_id']}\")\n",
    "        print(f\"Total sessions: {result['total_sessions']}\")\n",
    "        print(f\"Missing target sessions: {result['missing_targets']}\")\n",
    "        print(f\"All missing sessions: {result['all_missing']}\")\n",
    "        print(f\"Present sessions: {result['sessions']}\")\n",
    "    \n",
    "    # Summary of missing sessions\n",
    "    missing_counts = defaultdict(int)\n",
    "    for result in results:\n",
    "        for session in result['missing_targets']:\n",
    "            missing_counts[session] += 1\n",
    "    \n",
    "    print(\"\\nSummary of missing target sessions:\")\n",
    "    for session, count in sorted(missing_counts.items(), key=lambda x: int(x[0]) if x[0].isdigit() else x[0]):\n",
    "        print(f\"Session {session}: missing in {count} results\")\n",
    "\n",
    "# To use these functions:\n",
    "# 1. If you need to load the match.pkl file:\n",
    "\n",
    "# 2. If you already have the putative_cells loaded (from info or elsewhere):\n",
    "results = analyze_putative_cells(putative_cells, \n",
    "                                target_missing_sessions=[7, 8, 9], \n",
    "                                min_sessions=20)\n",
    "display_putative_cells_results(results)\n",
    "\n",
    "# 3. To directly access a specific cluster:\n",
    "# if results:\n",
    "#     cluster_idx = results[0]['cluster_idx']\n",
    "#     cluster = putative_cells[cluster_idx]\n",
    "#     print(f\"First result cluster has {len(cluster)} cells across different sessions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the zarr library and other necessary packages\n",
    "import zarr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Open the zarr file\n",
    "zarr_path = '/nrs/spruston/Gaby_imaging/raw/M54/multi_day_demix/vr2p.zarr'\n",
    "z = zarr.open(zarr_path, mode='r')\n",
    "\n",
    "# Define a function to process one entry of info at a time\n",
    "def analyze_id_session_in_item(item, item_index=None):\n",
    "    \"\"\"Process a single item to find ID-session relationships\"\"\"\n",
    "    print(f\"\\nAnalyzing item {item_index if item_index is not None else ''}\")\n",
    "    \n",
    "    # Create a mapping of IDs to sessions\n",
    "    id_to_sessions = defaultdict(set)\n",
    "    \n",
    "    # Direct approach: Look for patterns where both id and session are in the same dictionary\n",
    "    def find_id_session_pairs(obj, path=[]):\n",
    "        if isinstance(obj, dict):\n",
    "            # Check if this dictionary has both 'id' and 'session' keys\n",
    "            id_val = obj.get('id')\n",
    "            session_val = obj.get('session')\n",
    "            \n",
    "            if id_val is not None and session_val is not None:\n",
    "                id_to_sessions[str(id_val)].add(str(session_val))\n",
    "            \n",
    "            # Recursively check all values\n",
    "            for k, v in obj.items():\n",
    "                find_id_session_pairs(v, path + [k])\n",
    "                \n",
    "        elif isinstance(obj, list):\n",
    "            # Recursively check all items in the list\n",
    "            for i, item in enumerate(obj):\n",
    "                find_id_session_pairs(item, path + [f'[{i}]'])\n",
    "    \n",
    "    # Process the item\n",
    "    find_id_session_pairs(item)\n",
    "    \n",
    "    # Display results\n",
    "    if id_to_sessions:\n",
    "        print(f\"Found {len(id_to_sessions)} unique IDs across sessions\")\n",
    "        \n",
    "        # Show IDs with multiple sessions\n",
    "        multi_session_ids = {id_val: sessions for id_val, sessions in id_to_sessions.items() if len(sessions) > 1}\n",
    "        \n",
    "        if multi_session_ids:\n",
    "            print(f\"\\n{len(multi_session_ids)} IDs appear in multiple sessions:\")\n",
    "            \n",
    "            # Sort by number of sessions (most first)\n",
    "            for id_val, sessions in sorted(multi_session_ids.items(), key=lambda x: len(x[1]), reverse=True)[:10]:\n",
    "                print(f\"ID {id_val}: {len(sessions)} sessions - {sorted(sessions)}\")\n",
    "        else:\n",
    "            print(\"No IDs appear in multiple sessions\")\n",
    "    else:\n",
    "        print(\"No ID-session relationships found in this item\")\n",
    "    \n",
    "    return id_to_sessions\n",
    "\n",
    "# Function to process items one at a time\n",
    "def process_info_step_by_step(info):\n",
    "    all_mappings = []\n",
    "    \n",
    "    if isinstance(info, list):\n",
    "        print(f\"Info contains {len(info)} items\")\n",
    "        \n",
    "        # Process each item\n",
    "        for i, item in enumerate(info):\n",
    "            print(f\"\\n===== Processing Item {i} =====\")\n",
    "            mapping = analyze_id_session_in_item(item, i)\n",
    "            \n",
    "            if mapping:\n",
    "                all_mappings.append(mapping)\n",
    "                \n",
    "                # Display first few IDs and their sessions\n",
    "                print(\"\\nExample IDs and their sessions:\")\n",
    "                for id_val, sessions in list(mapping.items())[:5]:\n",
    "                    print(f\"ID {id_val}: {sorted(sessions)}\")\n",
    "            \n",
    "            # Simple progress indicator\n",
    "            if i < len(info) - 1:\n",
    "                print(f\"\\nProgress: {i+1}/{len(info)} items processed\")\n",
    "            \n",
    "            # Uncomment to pause between items:\n",
    "            # if i < len(info) - 1:\n",
    "            #     input(\"Press Enter to continue to next item...\")\n",
    "    \n",
    "    elif isinstance(info, dict):\n",
    "        print(\"Info is a dictionary (single item)\")\n",
    "        mapping = analyze_id_session_in_item(info)\n",
    "        \n",
    "        if mapping:\n",
    "            all_mappings.append(mapping)\n",
    "    \n",
    "    else:\n",
    "        print(f\"Info has unexpected type: {type(info)}\")\n",
    "    \n",
    "    return all_mappings\n",
    "\n",
    "# Use this function to analyze the loaded info variable\n",
    "# First make sure info is loaded from the zarr file\n",
    "# For example: info = z['path/to/info'][:]\n",
    "\n",
    "# Then run:\n",
    "# all_mappings = process_info_step_by_step(info)\n",
    "\n",
    "# Print summary information from all mappings\n",
    "def summarize_all_mappings(all_mappings):\n",
    "    if not all_mappings:\n",
    "        print(\"No mappings to summarize\")\n",
    "        return\n",
    "    \n",
    "    # Combine all mappings\n",
    "    combined = defaultdict(set)\n",
    "    for mapping in all_mappings:\n",
    "        for id_val, sessions in mapping.items():\n",
    "            combined[id_val].update(sessions)\n",
    "    \n",
    "    # Count sessions per ID\n",
    "    session_counts = {id_val: len(sessions) for id_val, sessions in combined.items()}\n",
    "    \n",
    "    # Find IDs with most sessions\n",
    "    top_ids = sorted(session_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nSummary across all {len(all_mappings)} processed items:\")\n",
    "    print(f\"Total unique IDs: {len(combined)}\")\n",
    "    \n",
    "    # Count unique sessions\n",
    "    all_sessions = set()\n",
    "    for sessions in combined.values():\n",
    "        all_sessions.update(sessions)\n",
    "    print(f\"Total unique sessions: {len(all_sessions)}\")\n",
    "    \n",
    "    # Top IDs with most sessions\n",
    "    print(\"\\nTop 10 IDs by number of sessions:\")\n",
    "    for id_val, count in top_ids[:10]:\n",
    "        sessions = combined[id_val]\n",
    "        print(f\"ID {id_val}: {count} sessions - {sorted(sessions)}\")\n",
    "    \n",
    "    # Distribution\n",
    "    count_distribution = defaultdict(int)\n",
    "    for _, count in session_counts.items():\n",
    "        count_distribution[count] += 1\n",
    "    \n",
    "    print(\"\\nDistribution of session counts:\")\n",
    "    for count, num_ids in sorted(count_distribution.items()):\n",
    "        print(f\"{count} session(s): {num_ids} IDs\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    counts = list(session_counts.values())\n",
    "    plt.hist(counts, bins=range(1, max(counts) + 2), alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Number of Sessions')\n",
    "    plt.ylabel('Number of IDs')\n",
    "    plt.title('Distribution of Number of Sessions per ID')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show()\n",
    "\n",
    "# To summarize after processing all items:\n",
    "# summarize_all_mappings(all_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info contains 5972 items\n",
      "Processing item 1...\n",
      "\n",
      "Processing item 1\n",
      "Found 28 'session' keys and 28 'id' keys\n",
      "\n",
      "Found 1 unique IDs across multiple sessions\n",
      "\n",
      "1 IDs appear in multiple sessions:\n",
      "ID 2: 28 sessions - ['1', '10', '12', '13', '14', '15', '19', '2', '21', '22', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '4', '5', '6', '7', '8', '9']\n",
      "\n",
      "Example of direct access to the mapping:\n",
      "ID 2 appears in 28 sessions: ['1', '10', '12', '13', '14', '15', '19', '2', '21', '22', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "# Import the functions from the analyze_sessions_ids.py script\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import zarr\n",
    "\n",
    "# Add the current directory to sys.path to ensure the script can be imported\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "\n",
    "# Import functions from the script\n",
    "from analyze_sessions_ids import find_keys, process_item, analyze_id_session_counts, visualize_results\n",
    "\n",
    "# Open the zarr file\n",
    "zarr_path = '/nrs/spruston/Gaby_imaging/raw/M54/multi_day_demix/vr2p.zarr'\n",
    "z = zarr.open(zarr_path, mode='r')\n",
    "info_orig = np.load('/nrs/spruston/Gaby_imaging/raw/M54/multi_day_demix/registration_data/match.pkl', allow_pickle=True)\n",
    "# Example of how to use the functions:\n",
    "# Assuming info is already loaded (as shown in your original message)\n",
    "# If not, you can load it from the zarr file depending on its location:\n",
    "\n",
    "# Option 1: If info is at the root level\n",
    "# info = z['info'][:]\n",
    "\n",
    "# Option 2: If info is nested inside another group\n",
    "# info = z['some_group']['info'][:]\n",
    "matched_cells = info_orig[0]\n",
    "# Process a single item from info\n",
    "if isinstance(info, list):\n",
    "    print(f\"Info contains {len(info)} items\")\n",
    "    \n",
    "    # Process the first item as an example\n",
    "    item_index = 1  # Change this to process different items\n",
    "    if len(info) > item_index:\n",
    "        print(f\"Processing item {item_index}...\")\n",
    "        item = info[item_index]\n",
    "        \n",
    "        # Find ID-session relationships in this item\n",
    "        mapping = process_item(item, item_index)\n",
    "        \n",
    "        # Analyze and display the results\n",
    "        id_session_map = analyze_id_session_counts(mapping)\n",
    "        \n",
    "        # If you want to visualize (this would normally be done after processing all items)\n",
    "        # visualize_results([id_session_map])\n",
    "        \n",
    "        # You can also access the mapping directly to inspect specific IDs\n",
    "        if id_session_map:\n",
    "            print(\"\\nExample of direct access to the mapping:\")\n",
    "            \n",
    "            # Get an example ID (first one in the mapping)\n",
    "            example_id = next(iter(id_session_map))\n",
    "            sessions = id_session_map[example_id]\n",
    "            \n",
    "            print(f\"ID {example_id} appears in {len(sessions)} sessions: {sorted(sessions)}\")\n",
    "    else:\n",
    "        print(f\"Index {item_index} is out of range (info has {len(info)} items)\")\n",
    "        \n",
    "elif isinstance(info, dict):\n",
    "    print(\"Info is a dictionary (single item)\")\n",
    "    mapping = process_item(info)\n",
    "    id_session_map = analyze_id_session_counts(mapping)\n",
    "    \n",
    "    # Example of direct access\n",
    "    if id_session_map:\n",
    "        example_id = next(iter(id_session_map))\n",
    "        sessions = id_session_map[example_id]\n",
    "        print(f\"ID {example_id} appears in {len(sessions)} sessions: {sorted(sessions)}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"Info has unexpected type: {type(info)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analysis of sessions and IDs in the nested info structure\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# First, let's define functions to analyze the structure\n",
    "def find_keys(obj, target_key, path=None, results=None):\n",
    "    \"\"\"Recursively search for keys in a nested dictionary/list structure\"\"\"\n",
    "    if path is None:\n",
    "        path = []\n",
    "    if results is None:\n",
    "        results = []\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            if k == target_key:\n",
    "                results.append((path + [k], v))\n",
    "            find_keys(v, target_key, path + [k], results)\n",
    "    elif isinstance(obj, list):\n",
    "        for i, item in enumerate(obj):\n",
    "            find_keys(item, target_key, path + [f\"[{i}]\"], results)\n",
    "            \n",
    "    return results\n",
    "\n",
    "def find_id_session_relationships(data):\n",
    "    \"\"\"Find all possible relationships between IDs and sessions\"\"\"\n",
    "    # Create mappings for IDs to sessions\n",
    "    id_to_sessions = defaultdict(set)\n",
    "    \n",
    "    # Method 1: Find keys and path relationships\n",
    "    session_results = find_keys(data, 'session')\n",
    "    id_results = find_keys(data, 'id')\n",
    "    \n",
    "    print(f\"Found {len(session_results)} 'session' keys\")\n",
    "    print(f\"Found {len(id_results)} 'id' keys\")\n",
    "    \n",
    "    # Try to find relationships based on path proximity\n",
    "    for id_path, id_value in id_results:\n",
    "        id_path_str = '.'.join(map(str, id_path[:-1]))\n",
    "        \n",
    "        # Convert to string for consistent handling\n",
    "        id_value_str = str(id_value)\n",
    "        \n",
    "        for session_path, session_value in session_results:\n",
    "            session_path_str = '.'.join(map(str, session_path[:-1]))\n",
    "            session_value_str = str(session_value)\n",
    "            \n",
    "            # Check if they might be related (in same object or parent/child relationship)\n",
    "            common_prefix = os.path.commonprefix([id_path_str, session_path_str])\n",
    "            if common_prefix and len(common_prefix) > 1:  # They share a common parent\n",
    "                id_to_sessions[id_value_str].add(session_value_str)\n",
    "    \n",
    "    # Method 2: If data has specific patterns we can directly extract\n",
    "    # For example, if it's organized by session with IDs inside\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            # Pattern: {'session1': {'ids': [1, 2, 3]}, 'session2': {'ids': [2, 4, 5]}}\n",
    "            if isinstance(value, dict) and 'ids' in value and isinstance(value['ids'], list):\n",
    "                for id_val in value['ids']:\n",
    "                    id_to_sessions[str(id_val)].add(str(key))\n",
    "                    \n",
    "            # Pattern: {'cells': [{'id': 1, 'session': 'A'}, {'id': 2, 'session': 'B'}]}\n",
    "            if key == 'cells' and isinstance(value, list):\n",
    "                for cell in value:\n",
    "                    if isinstance(cell, dict) and 'id' in cell and 'session' in cell:\n",
    "                        id_to_sessions[str(cell['id'])].add(str(cell['session']))\n",
    "    \n",
    "    # Method 3: If data has components with both session and id\n",
    "    components = []\n",
    "    if isinstance(data, dict):\n",
    "        # Try to find all individual components/items that might have both id and session\n",
    "        if 'components' in data and isinstance(data['components'], list):\n",
    "            components = data['components']\n",
    "        elif 'cells' in data and isinstance(data['cells'], list):\n",
    "            components = data['cells']\n",
    "        elif 'items' in data and isinstance(data['items'], list):\n",
    "            components = data['items']\n",
    "        \n",
    "        # Check each component for id and session\n",
    "        for comp in components:\n",
    "            if isinstance(comp, dict):\n",
    "                id_val = None\n",
    "                session_val = None\n",
    "                \n",
    "                # Look for id and session keys or their variations\n",
    "                for key in comp:\n",
    "                    if key.lower() == 'id' or key.lower().endswith('_id'):\n",
    "                        id_val = comp[key]\n",
    "                    if key.lower() == 'session' or key.lower().endswith('_session'):\n",
    "                        session_val = comp[key]\n",
    "                \n",
    "                if id_val is not None and session_val is not None:\n",
    "                    id_to_sessions[str(id_val)].add(str(session_val))\n",
    "    \n",
    "    return id_to_sessions\n",
    "\n",
    "# Function to analyze and display the results\n",
    "def analyze_id_session_counts(mapping):\n",
    "    \"\"\"Analyze and display counts of sessions per ID\"\"\"\n",
    "    if not mapping:\n",
    "        print(\"No ID-session relationships found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(mapping)} unique IDs across multiple sessions\")\n",
    "    \n",
    "    # Count sessions per ID\n",
    "    session_counts = {id_val: len(sessions) for id_val, sessions in mapping.items()}\n",
    "    \n",
    "    # Sort IDs by number of sessions (descending)\n",
    "    sorted_ids = sorted(session_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Display IDs with most sessions\n",
    "    print(\"\\nTop 10 IDs by number of sessions:\")\n",
    "    for id_val, count in sorted_ids[:10]:\n",
    "        sessions = mapping[id_val]\n",
    "        print(f\"ID {id_val}: {count} sessions - {sorted(sessions)}\")\n",
    "    \n",
    "    # Display distribution\n",
    "    count_distribution = defaultdict(int)\n",
    "    for _, count in session_counts.items():\n",
    "        count_distribution[count] += 1\n",
    "    \n",
    "    print(\"\\nDistribution of session counts:\")\n",
    "    for count, num_ids in sorted(count_distribution.items()):\n",
    "        print(f\"{count} session(s): {num_ids} IDs\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    all_counts = list(session_counts.values())\n",
    "    avg_count = sum(all_counts) / len(all_counts)\n",
    "    max_count = max(all_counts)\n",
    "    min_count = min(all_counts)\n",
    "    \n",
    "    print(\"\\nStatistics:\")\n",
    "    print(f\"Average sessions per ID: {avg_count:.2f}\")\n",
    "    print(f\"Maximum sessions per ID: {max_count}\")\n",
    "    print(f\"Minimum sessions per ID: {min_count}\")\n",
    "    \n",
    "    # Count unique sessions\n",
    "    all_sessions = set()\n",
    "    for sessions in mapping.values():\n",
    "        all_sessions.update(sessions)\n",
    "    \n",
    "    print(f\"Total unique sessions: {len(all_sessions)}\")\n",
    "    \n",
    "    return sorted_ids, all_sessions\n",
    "\n",
    "# Now let's analyze the info variable\n",
    "import os  # Needed for common prefix calculations\n",
    "print(\"Analyzing the info structure...\")\n",
    "id_session_mapping = find_id_session_relationships(info)\n",
    "top_ids, all_sessions = analyze_id_session_counts(id_session_mapping)\n",
    "\n",
    "# Visualize the distribution of sessions per ID\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if id_session_mapping:\n",
    "    # Count sessions per ID\n",
    "    session_counts = [len(sessions) for sessions in id_session_mapping.values()]\n",
    "    \n",
    "    # Create a histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(session_counts, bins=range(1, max(session_counts) + 2), alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Number of Sessions')\n",
    "    plt.ylabel('Number of IDs')\n",
    "    plt.title('Distribution of Number of Sessions per ID')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.xticks(range(1, max(session_counts) + 1))\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a pie chart for distribution\n",
    "    count_distribution = defaultdict(int)\n",
    "    for count in session_counts:\n",
    "        count_distribution[count] += 1\n",
    "    \n",
    "    # For pie chart, group small slices\n",
    "    threshold = 0.03  # Minimum percentage to show as separate slice\n",
    "    total_ids = len(session_counts)\n",
    "    \n",
    "    pie_data = {}\n",
    "    other_count = 0\n",
    "    \n",
    "    for count, num_ids in count_distribution.items():\n",
    "        percentage = num_ids / total_ids\n",
    "        if percentage >= threshold:\n",
    "            pie_data[f\"{count} session(s)\"] = num_ids\n",
    "        else:\n",
    "            other_count += num_ids\n",
    "    \n",
    "    if other_count > 0:\n",
    "        pie_data[\"Other\"] = other_count\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.pie(pie_data.values(), labels=pie_data.keys(), autopct='%1.1f%%', \n",
    "            startangle=90, shadow=True, explode=[0.05] * len(pie_data))\n",
    "    plt.axis('equal')\n",
    "    plt.title('Distribution of IDs by Number of Sessions')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: Directly mapping IDs to sessions\n",
    "# This is especially useful if your data structure has a clear pattern\n",
    "\n",
    "def extract_id_session_mapping(data):\n",
    "    \"\"\"\n",
    "    Extract a mapping from IDs to sessions based on the specific structure of your data.\n",
    "    This function needs to be customized based on your actual data structure.\n",
    "    \"\"\"\n",
    "    id_to_sessions = defaultdict(set)\n",
    "    \n",
    "    # Assuming the structure might be something like:\n",
    "    # data = {\n",
    "    #    'sessions': [\n",
    "    #        {'id': 123, 'session': 'A'},\n",
    "    #        {'id': 123, 'session': 'B'},\n",
    "    #        {'id': 456, 'session': 'A'},\n",
    "    #        ...\n",
    "    #    ]\n",
    "    # }\n",
    "    \n",
    "    # Option 1: If data has a list of dictionaries with both 'id' and 'session' keys\n",
    "    if isinstance(data, dict) and 'sessions' in data and isinstance(data['sessions'], list):\n",
    "        for item in data['sessions']:\n",
    "            if 'id' in item and 'session' in item:\n",
    "                id_to_sessions[item['id']].add(item['session'])\n",
    "    \n",
    "    # Option 2: If data is structured with sessions as keys and lists of IDs as values\n",
    "    # e.g., {'session1': [id1, id2], 'session2': [id2, id3]}\n",
    "    elif isinstance(data, dict):\n",
    "        for session, ids in data.items():\n",
    "            if isinstance(ids, list):\n",
    "                for id_val in ids:\n",
    "                    id_to_sessions[id_val].add(session)\n",
    "    \n",
    "    # Option 3: For more complex structures, we might need to navigate through the hierarchy\n",
    "    # Printing the first few levels of your data structure might help identify patterns\n",
    "    else:\n",
    "        print(\"Structure not recognized. Printing data structure:\")\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in list(data.items())[:5]:  # Print first 5 items\n",
    "                print(f\"{key}: {type(value)}\")\n",
    "                if isinstance(value, (dict, list)) and len(str(value)) > 100:\n",
    "                    print(f\"  Sample: {str(value)[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"  Value: {value}\")\n",
    "        elif isinstance(data, list):\n",
    "            for i, item in enumerate(data[:5]):  # Print first 5 items\n",
    "                print(f\"[{i}]: {type(item)}\")\n",
    "                if isinstance(item, (dict, list)) and len(str(item)) > 100:\n",
    "                    print(f\"  Sample: {str(item)[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"  Value: {item}\")\n",
    "        else:\n",
    "            print(f\"Data type: {type(data)}\")\n",
    "    \n",
    "    return id_to_sessions\n",
    "\n",
    "# Try the direct mapping approach\n",
    "print(\"Trying direct mapping approach...\")\n",
    "direct_mapping = extract_id_session_mapping(info)\n",
    "\n",
    "if direct_mapping:\n",
    "    print(f\"\\nFound mapping with {len(direct_mapping)} unique IDs\")\n",
    "    \n",
    "    # Display IDs with multiple sessions\n",
    "    multi_session_ids = {id_val: sessions for id_val, sessions in direct_mapping.items() if len(sessions) > 1}\n",
    "    print(f\"IDs in multiple sessions: {len(multi_session_ids)}\")\n",
    "    \n",
    "    if multi_session_ids:\n",
    "        print(\"\\nTop 10 IDs with most sessions:\")\n",
    "        for id_val, sessions in sorted(multi_session_ids.items(), key=lambda x: len(x[1]), reverse=True)[:10]:\n",
    "            print(f\"ID {id_val}: {len(sessions)} sessions - {sorted(sessions)}\")\n",
    "else:\n",
    "    print(\"No mapping found with direct approach. The data structure might need more specific handling.\")\n",
    "    \n",
    "    # Let's print more detailed information about the structure to help customize the code\n",
    "    print(\"\\nAttempting to identify the structure pattern...\")\n",
    "    \n",
    "    # Recursive function to show the structure pattern with a limited depth\n",
    "    def show_structure_pattern(obj, depth=0, max_depth=3):\n",
    "        prefix = \"  \" * depth\n",
    "        if depth >= max_depth:\n",
    "            return f\"{prefix}...\"\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            result = f\"{prefix}dict with keys: {list(obj.keys())}\"\n",
    "            if depth < max_depth - 1 and obj:\n",
    "                sample_key = next(iter(obj))\n",
    "                result += f\"\\n{prefix}Sample for key '{sample_key}':\\n\"\n",
    "                result += show_structure_pattern(obj[sample_key], depth + 1, max_depth)\n",
    "            return result\n",
    "        elif isinstance(obj, list):\n",
    "            result = f\"{prefix}list with {len(obj)} items\"\n",
    "            if depth < max_depth - 1 and obj:\n",
    "                result += f\"\\n{prefix}Sample for first item:\\n\"\n",
    "                result += show_structure_pattern(obj[0], depth + 1, max_depth)\n",
    "            return result\n",
    "        else:\n",
    "            return f\"{prefix}value of type {type(obj).__name__}\"\n",
    "    \n",
    "    # Show the structure pattern of the info variable\n",
    "    print(show_structure_pattern(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the nested info structure to find sessions and IDs\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to recursively search for keys in a nested dictionary\n",
    "def find_keys(obj, target_key, path=None, results=None):\n",
    "    if path is None:\n",
    "        path = []\n",
    "    if results is None:\n",
    "        results = []\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            if k == target_key:\n",
    "                results.append((path + [k], v))\n",
    "            find_keys(v, target_key, path + [k], results)\n",
    "    elif isinstance(obj, list):\n",
    "        for i, item in enumerate(obj):\n",
    "            find_keys(item, target_key, path + [f\"[{i}]\"], results)\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Find all 'session' keys and values\n",
    "session_results = find_keys(info, 'session')\n",
    "print(f\"Found {len(session_results)} 'session' keys\")\n",
    "\n",
    "# Find all 'id' keys and values\n",
    "id_results = find_keys(info, 'id')\n",
    "print(f\"Found {len(id_results)} 'id' keys\")\n",
    "\n",
    "# Create a mapping from ID to sessions\n",
    "id_to_sessions = defaultdict(set)\n",
    "\n",
    "# Extract session identifiers and IDs\n",
    "# This approach depends on how your data is structured\n",
    "# We'll need to extract the session information and match it with IDs\n",
    "# This is a generalized approach that might need adjustment based on your actual data structure\n",
    "\n",
    "# Option 1: If IDs and sessions are at the same level in the hierarchy\n",
    "for path, session_value in session_results:\n",
    "    # Try to find an ID in the same parent dictionary\n",
    "    parent_path = path[:-1]  # Get the parent path\n",
    "    parent_str = '.'.join(map(str, parent_path))\n",
    "    \n",
    "    for id_path, id_value in id_results:\n",
    "        id_parent_path = id_path[:-1]\n",
    "        id_parent_str = '.'.join(map(str, id_parent_path))\n",
    "        \n",
    "        # If they share the same parent, link them\n",
    "        if parent_str == id_parent_str:\n",
    "            id_to_sessions[id_value].add(session_value)\n",
    "\n",
    "# Option 2: If the structure is more complex, we might need a different approach\n",
    "# For example, if sessions contain IDs or vice versa\n",
    "for path, session_value in session_results:\n",
    "    # Try to extract session identifier - this depends on your data structure\n",
    "    session_id = str(session_value)  # Adjust based on your data\n",
    "    \n",
    "    # Look through all paths leading to this session\n",
    "    session_parent = '.'.join(map(str, path[:-1]))\n",
    "    \n",
    "    # Find IDs that might be related to this session\n",
    "    for id_path, id_value in id_results:\n",
    "        id_parent = '.'.join(map(str, id_path[:-1]))\n",
    "        \n",
    "        # Check if the ID is within the session structure or vice versa\n",
    "        # This is a simplistic check - you might need to adjust based on your data\n",
    "        if session_parent in id_parent or id_parent in session_parent:\n",
    "            id_to_sessions[id_value].add(session_id)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nNumber of sessions per ID:\")\n",
    "for id_val, sessions in id_to_sessions.items():\n",
    "    print(f\"ID {id_val}: {len(sessions)} sessions - {sorted(sessions)}\")\n",
    "\n",
    "# Show IDs with multiple sessions\n",
    "print(\"\\nIDs appearing in multiple sessions:\")\n",
    "multi_session_ids = {id_val: sessions for id_val, sessions in id_to_sessions.items() if len(sessions) > 1}\n",
    "for id_val, sessions in sorted(multi_session_ids.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "    print(f\"ID {id_val}: {len(sessions)} sessions - {sorted(sessions)}\")\n",
    "\n",
    "# Calculate statistics\n",
    "if id_to_sessions:\n",
    "    session_counts = [len(sessions) for sessions in id_to_sessions.values()]\n",
    "    avg_sessions = sum(session_counts) / len(session_counts)\n",
    "    max_sessions = max(session_counts)\n",
    "    min_sessions = min(session_counts)\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"Average sessions per ID: {avg_sessions:.2f}\")\n",
    "    print(f\"Maximum sessions per ID: {max_sessions}\")\n",
    "    print(f\"Minimum sessions per ID: {min_sessions}\")\n",
    "    print(f\"Total unique IDs: {len(id_to_sessions)}\")\n",
    "    \n",
    "    # Count IDs by number of sessions\n",
    "    session_count_distribution = defaultdict(int)\n",
    "    for count in session_counts:\n",
    "        session_count_distribution[count] += 1\n",
    "    \n",
    "    print(\"\\nDistribution of IDs by number of sessions:\")\n",
    "    for count, num_ids in sorted(session_count_distribution.items()):\n",
    "        print(f\"{count} session(s): {num_ids} IDs\")\n",
    "else:\n",
    "    print(\"No ID-session relationships found with the current approach.\")\n",
    "    print(\"You may need to adjust the code based on the specific structure of your data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Zarr File Contents\n",
    "\n",
    "This notebook will open and explore the zarr file at `/nrs/spruston/Gaby_imaging/raw/M54/multi_day_demix/vr2p.zarr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import zarr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the zarr file\n",
    "zarr_path = '/nrs/spruston/Gaby_imaging/raw/M54/multi_day_demix/vr2p.zarr'\n",
    "z = zarr.open(zarr_path, mode='r')\n",
    "\n",
    "# Display the root group\n",
    "print(\"Root group structure:\")\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all groups and arrays in the zarr file\n",
    "def explore_zarr(group, path=''):\n",
    "    \"\"\"Recursively explore zarr group structure\"\"\"\n",
    "    for key in group.keys():\n",
    "        item_path = f\"{path}/{key}\"\n",
    "        item = group[key]\n",
    "        \n",
    "        if isinstance(item, zarr.Group):\n",
    "            print(f\"Group: {item_path}\")\n",
    "            explore_zarr(item, item_path)\n",
    "        elif isinstance(item, zarr.Array):\n",
    "            print(f\"Array: {item_path}, Shape: {item.shape}, Dtype: {item.dtype}, Chunks: {item.chunks}\")\n",
    "\n",
    "print(\"Full zarr structure:\")\n",
    "explore_zarr(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attributes if available\n",
    "print(\"\\nRoot attributes:\")\n",
    "try:\n",
    "    for key, value in z.attrs.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "except AttributeError:\n",
    "    print(\"No attributes found at root level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional example: Examining a specific chunk of data\n",
    "\"\"\"\n",
    "# If you have a large dataset, you might want to examine specific chunks\n",
    "# array_path = 'example_array_path'  # Replace with actual path\n",
    "# array = z[array_path]\n",
    "\n",
    "# # Determine chunk size and shape\n",
    "# print(f\"Chunk shape: {array.chunks}\")\n",
    "\n",
    "# # Access a specific chunk (for a 2D array)\n",
    "# if len(array.shape) == 2:\n",
    "#     # Get a specific chunk - adjust indices based on your data\n",
    "#     chunk_data = array[0:array.chunks[0], 0:array.chunks[1]]\n",
    "#     \n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     plt.imshow(chunk_data)\n",
    "#     plt.colorbar()\n",
    "#     plt.title(f'First chunk of {array_path}')\n",
    "#     plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python38 vr2p",
   "language": "python",
   "name": "vr2p"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
